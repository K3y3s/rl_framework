{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn.nn import NN\n",
    "import gymnasium as gym\n",
    "from agent.agent import AgentBuilder, ActionContext\n",
    "from experience.experience import ExperienceBuffer\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'FrozenLake-v1'\n",
    "EPSILON = 1\n",
    "EPSILON_DECAY = 1000\n",
    "EPSILON_DECAY_VALUE = 0 \n",
    "POLICY = 'GREEDY'\n",
    "ACTION_METHOD = 'NEURAL_NETWORK'\n",
    "EXPERIENCE_BUFFER = 10\n",
    "SAMPLE_SIZE = 10\n",
    "LEARNING_RATE = 0.01\n",
    "GAMMA = 0.1\n",
    "HIDDEN_DIMENSION = (16, 16)\n",
    "A_FUNCTION = torch.nn.ReLU()\n",
    "LOSS_F=torch.nn.MSELoss()\n",
    "OPTIMIZER=torch.optim.Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create environment to play with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "env = gym.make(ENV_NAME, desc=None, map_name=\"4x4\", is_slippery=False,\n",
    "                render_mode=\"rgb_array\")\n",
    "starting_state, _ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_frame(env_, number):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 5))\n",
    "    ax.imshow(env_.render())\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(number)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create prediction/on-line neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nn = NN(\n",
    "    nb_of_state_variables = 1,\n",
    "    nb_of_actions = env.action_space.n,\n",
    "    activation_functions= A_FUNCTION,\n",
    "    hidden_dims=HIDDEN_DIMENSION\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some theory:\n",
    "\n",
    "Assumption for supervised learning with gradient descent:\n",
    "1) data must by independent\n",
    "2) identically distributed\n",
    "3) target needs to be stationary\n",
    "However for reinforcement learning these assumption are not kept.\n",
    "\n",
    "Three classic optimization technics in ML:\n",
    "1) Batch gradient descent -> taking all samples at once. Not practical -> to large data sets\n",
    "2) Mini batch gradient descent -> taking small portion of samples\n",
    "3) Stochastic gradient descent -> taking one sample per step. High variance per step of optimization. Less stable then mini-batch gradient descent\n",
    "The bigger the sample space, the lower the variance of the steps. To large sample to slow process of learning. In order to use mini-batch gradient descent experience is gathered.\n",
    "\n",
    "Problems with classical RL methods:\n",
    "\n",
    "* Problem 1 - Collected samples comes from the same trajectory and policy. Experience gathered in state t+1 depends on the state t. So experience will looks like (t -> t+1 -> t+2 -> t+3). In each time information about variables of a current state, variables of a previous state and reward are kept. When target network is updated and then samples are collected a mini-batch comes from older policy. What's more batches are correlated with each other. It violate assumptions that data are independent (correlation between batches) and identically distributed (policy is changing so the samples distribution are changing)\n",
    "\n",
    "\n",
    "* Problem 2 - Weights are optimized for all states at once. When using the same target network and optimization network it creates non-stationary target. This means that the values estimated by a target network maybe outdated. Moreover target shifts its values when the network weights are updated.\n",
    "\n",
    "Value_estimated_by_neural_network_for_current_state = Reward + gamma * max(Value_estimated_by_neural_network_for_next_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving issues with IID:\n",
    "Solution to the Problem 1: Using batches -> Collecting data with use of the same neural network. Improvements to that solution could be use of replay buffer. Having ExperienceBuffer from which samples are taken much higher than batches use for learning. Old diverse experience can halp learning. Samples are information about state and action and consequence of this action (variables describing next state and reward).\n",
    "\n",
    "Solution with a framework: \\\n",
    "size_of_buffer >> size_of_sample\n",
    "\n",
    "eg. \\\n",
    "    exp = ExperienceBuffer(size_of_buffer=1000)\n",
    "    samples = exp.get_samples(100)\n",
    "\n",
    "Important: Neural Networks are used to estimate consequences of a transition. Loss function is used to evaluate how consequences were estimated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = ExperienceBuffer(\n",
    "    size_of_buffer=EXPERIENCE_BUFFER, \n",
    "    sampling_method='SIMPLE_RANDOM'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution to the Problem 2:  Using different target network -> it tends to solve problems with non-stationary of a target. One network is used to evaluate value of a target (Value_estimated_by_neural_network_for_next_state) and another is used to optimize its weights to better estimate consequences of a transitions between two states after taking action. After several iteration weights of both neural networks are synchronized. \n",
    "\n",
    "Remark: This are not really a different network. The architecture is the same only weights are different.\n",
    "\n",
    "In order to use different target network synchronization attribute must be set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "action_context = ActionContext(\n",
    "    actions_number=env.action_space.n,\n",
    "    action_methods=ACTION_METHOD,\n",
    "    epsilon = EPSILON,\n",
    "    nn = nn,\n",
    "    policy= POLICY,\n",
    "    epsilon_decay=EPSILON_DECAY,\n",
    "    epsilon_decay_value=EPSILON_DECAY_VALUE\n",
    ")\n",
    "\n",
    "agent = AgentBuilder(\n",
    "    action_context,\n",
    "    env=env,\n",
    "    gamma=GAMMA,\n",
    "    loss_function=LOSS_F,\n",
    "    optimizer=OPTIMIZER,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    synchronization = SAMPLE_SIZE # after how many step synchronization will be made\n",
    "    ).build_agent()\n",
    "\n",
    "agent.trigger_decay = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step nb: 0\n",
      "Episode ended after: 7, Reward = 1.0\n",
      "Step nb: 10000\n",
      "Step nb: 20000\n",
      "Step nb: 30000\n",
      "Step nb: 40000\n",
      "Step nb: 50000\n",
      "Step nb: 60000\n",
      "Step nb: 70000\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "episode_steps = 0\n",
    "\n",
    "episode_steps_all = []\n",
    "while counter <= 100000:\n",
    "    \n",
    "    if counter % 10000 == 0:\n",
    "        print(f'Step nb: {counter}')\n",
    "    \n",
    "    # gathering experience\n",
    "    starting_state = torch.tensor([starting_state], dtype=torch.float)\n",
    "    starting_state.unsqueeze(0)\n",
    "    \n",
    "    action = agent.compute_action(starting_state)\n",
    "    exp_s = agent.make_step(action, starting_state[0].item())\n",
    "    #print(f'state: {exp_s.state}, previous_state: {exp_s.previous_state}, action: {exp_s.action}')\n",
    "    #print_frame(agent.env, counter)\n",
    "    exp.append(exp_s)\n",
    "    starting_state = exp_s.state\n",
    "    if exp_s.terminated:\n",
    "        starting_state, _ = agent.env.reset()\n",
    "        if exp_s.reward == 1:\n",
    "            agent.trigger_decay = True\n",
    "            print(f'Episode ended after: {episode_steps}, Reward = {exp_s.reward}')\n",
    "            \n",
    "        episode_steps_all.append(episode_steps)\n",
    "        episode_steps = 0\n",
    "        \n",
    "    \n",
    "    # train agent\n",
    "    if len(exp) > SAMPLE_SIZE:\n",
    "        samples = exp.get_samples(SAMPLE_SIZE)\n",
    "        agent.train_agent(samples)\n",
    "        \n",
    "    # synchronization\n",
    "    agent.synchronize(counter)\n",
    "    \n",
    "    counter +=1\n",
    "    episode_steps +=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "win_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
